{
    "label": "Llama CPP",
    "description": "Handler for loading Llama CPP models.",
    "unique_key": "llama_cpp",
    "script": "llama-cpp.py",
    "skills": [        
        {
            "label":  "Llama2 7B Chat (GGUF)",
            "routing_key": "llama_7b_chat_gguf",
            "use": ["language_model"],        
            "available_precision": { "cpu": ["4-bit", "6-bit"], "cuda": ["4-bit", "6-bit"] },
            "memory_usage": { "4-bit": 6000, "6-bit": 8000 },            
            "model": [{
                "name": "TheBloke/Llama-2-7B-chat-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "llama-2-7b-chat.Q4_0.gguf",
                    "5-bit": "llama-2-7b-chat.Q5_0.gguf",
                    "6-bit": "llama-2-7b-chat.Q6_K.gguf"
                }
            }],
            "configuration": {
                "model_layers": 43,
                "num_threads": -1,
                "model_type": "chat",
                "max_seq_len": 4096,
                "user_role": "[INST]",
                "ai_role": "[/INST]",
                "stop_on": ["<stop>", "[INST]", "</s>"],
                "system_prompt_format": "{user_role} <<SYS>>{system_prompt}<</SYS>>\n{prompt} {ai_role} {response}",
                "system_message": "You are an helpful assistant.",
                "prompt_format": "{user_role} {prompt} {ai_role} {response}"
            }        
        },
        {
            "label":  "Llama2 13B Chat (GGUF)",
            "routing_key": "llama_13b_chat_gguf",            
            "use": ["language_model"],        
            "available_precision": { "cpu": ["4-bit", "6-bit"], "cuda": ["4-bit", "6-bit"] },
            "memory_usage": { "4-bit": 11000, "6-bit": 13800 },            
            "model": [{
                "name": "TheBloke/Llama-2-13B-chat-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "llama-2-13b-chat.Q4_0.gguf",
                    "6-bit": "llama-2-13b-chat.Q6_K.gguf"
                }
            }],
            "configuration": {
                "model_layers": 43,
                "num_threads": -1,
                "model_type": "chat",
                "max_seq_len": 4096,
                "user_role": "[INST]",
                "ai_role": "[/INST]",
                "stop_on": ["<stop>", "[INST]", "</s>"],
                "system_prompt_format": "{user_role} <<SYS>>{system_prompt}<</SYS>>\n{prompt} {ai_role} {response}",
                "system_message": "You are an helpful assistant.",
                "prompt_format": "{user_role} {prompt} {ai_role} {response}"
            }        
        },
        {
            "label":  "CodeLlama 34B Instruct (GGUF)",
            "routing_key": "llama_34b_instruct_gguf",            
            "use": ["language_model"],        
            "available_precision": { "cpu": ["4-bit", "6-bit"], "cuda": ["4-bit", "6-bit"] },
            "memory_usage": { "4-bit": 22000, "6-bit": 31500 },            
            "model": [{
                "name": "TheBloke/CodeLlama-34B-Instruct-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "codellama-34b-instruct.Q4_K_M.gguf",
                    "6-bit": "codellama-34b-instruct.Q6_K.gguf"
                }
            }],
            "configuration": {
                "model_layers": 51,
                "num_threads": -1,
                "model_type": "instruct",
                "max_seq_len": 16384,
                "user_role": "[INST]",
                "ai_role": "[/INST]",
                "stop_on": ["<stop>", "[INST]", "</s>"],
                "system_message": "",
                "prompt_format": "{user_role} {prompt} {ai_role} {response}\n\n"
            }        
        },
        {
            "label":  "Mistral 7B (GGUF)",
            "routing_key": "mistral_7b_gguf",            
            "use": ["language_model"],                    
            "special_ability": ["foundational"],
            "available_precision": { "cuda": ["4-bit", "6-bit"] },
            "memory_usage": { "4-bit": 6900, "6-bit": 11000 },
            "model": [{
                "name": "TheBloke/Mistral-7B-v0.1-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "mistral-7b-v0.1.Q4_K_M.gguf",
                    "6-bit": "mistral-7b-v0.1.Q6_K.gguf"
                }
            }],
            "configuration": {           
                "model_layers": 35,     
                "max_seq_len": 8192,
                "user_role": "### USER:",
                "ai_role": "### ASSISTANT:",
                "stop_on": ["<stop>", "### USER:"],
                "system_prompt_format": "### SYSTEM: {system_prompt}\n\n",
                "default_lora": "noco-ai/multishot-mistral-7b",
                "system_message": "You are a AI chatbot, answer all the users questions to the best of your ability.",
                "prompt_format": "{user_role} {prompt}\n{ai_role} {response}\n"
            }
        },        
        {
            "label":  "Yi 34B 200k (GGUF)",
            "routing_key": "yi_34b_gguf",            
            "use": ["language_model"],        
            "available_precision": { "cpu": ["4-bit", "6-bit"], "cuda": ["4-bit", "6-bit"] },
            "memory_usage": { "4-bit": 22000, "6-bit": 31500 },            
            "model": [{
                "name": "TheBloke/Yi-34B-200K-Llamafied-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "yi-34b-200k-llamafied.Q4_K_M.gguf",
                    "6-bit": "yi-34b-200k-llamafied.Q6_K.gguf"
                }
            }],
            "configuration": {
                "default_lora": "noco-ai/yaya",
                "model_layers": 63,
                "num_threads": -1,
                "max_seq_len": 16384,
                "user_role": "",
                "ai_role": "",
                "stop_on": ["<stop>", "USER:", "</s>"],
                "system_message": "",
                "prompt_format": "{user_role} {prompt}\n{ai_role} {response}\n\n"
            }        
        },
        {
            "label":  "Nous Capybara 34B (GGUF)",
            "routing_key": "nous_capybara_gguf",            
            "use": ["language_model"],        
            "available_precision": { "cpu": ["4-bit", "6-bit"], "cuda": ["4-bit", "6-bit"] },
            "memory_usage": { "4-bit": 22000, "6-bit": 31500 },            
            "model": [{
                "name": "TheBloke/Nous-Capybara-34B-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "nous-capybara-34b.Q4_K_M.gguf",
                    "6-bit": "nous-capybara-34b.Q6_K.gguf"
                }
            }],
            "configuration": {
                "model_layers": 63,
                "num_threads": -1,
                "max_seq_len": 16384,
                "user_role": "USER:",
                "ai_role": "ASSISTANT:",
                "stop_on": ["<stop>", "USER:", "</s>"],
                "system_message": "",
                "prompt_format": "{user_role} {prompt}\n{ai_role} {response}\n\n"
            }        
        },
        {
            "label":  "Llama2 70B Chat (GGUF)",
            "routing_key": "llama_70b_chat_gguf",            
            "use": ["language_model"],        
            "available_precision": { "cpu": ["4-bit", "5-bit", "6-bit"], "cuda": ["4-bit", "5-bit", "6-bit"] },
            "memory_usage": { "4-bit": 41500, "5-bit": 48000, "6-bit": 60000 },            
            "model": [{
                "name": "TheBloke/Llama-2-70B-chat-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "llama-2-70b-chat.Q4_K_M.gguf",
                    "5-bit": "llama-2-70b-chat.Q5_K_M.gguf",
                    "6-bit": "llama-2-70b-chat.Q6_K.gguf"
                },
                "split": {
                    "6-bit": ["-split-a", "-split-b"]
                }
            }],
            "configuration": {
                "model_layers": 83,
                "num_threads": -1,
                "model_type": "chat",
                "max_seq_len": 4096,
                "user_role": "[INST]",
                "ai_role": "[/INST]",
                "stop_on": ["<stop>", "[INST]", "</s>"],
                "system_prompt_format": "{user_role} <<SYS>>{system_prompt}<</SYS>>\n{prompt} {ai_role} {response}",
                "prompt_format": "{user_role} {prompt} {ai_role} {response}",
                "system_message": "You are an helpful assistant."
            }        
        }
    ]    
}
{
    "label": "Llama CPP",
    "description": "Handler for loading Llama CPP models.",
    "unique_key": "llama_cpp",
    "script": "llama-cpp.py",
    "skills": [        
        {
            "label":  "Llama2 7B Chat (GGUF)",
            "routing_key": "llama_7b_chat_gguf",
            "use": ["language_model"],        
            "available_precision": { "cpu": ["4-bit", "6-bit"], "cuda": ["4-bit", "6-bit"] },
            "memory_usage": { "4-bit": 6000, "6-bit": 8000 },            
            "model": [{
                "name": "TheBloke/Llama-2-7B-chat-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "llama-2-7b-chat.Q4_0.gguf"
                }
            }],
            "configuration": {
                "model_layers": 43,
                "num_threads": -1,
                "model_type": "chat",
                "max_seq_len": 4096,
                "user_role": "[INST]",
                "ai_role": "[/INST]",
                "stop_on": ["<stop>", "[INST]", "</s>"],
                "system_prompt_format": "{user_role} <<SYS>>{system_prompt}<</SYS>>\n{prompt} {ai_role} {response}",
                "system_message": "You are an helpful assistant.",
                "prompt_format": "{user_role} {prompt} {ai_role} {response}"
            }        
        },
        {
            "label":  "Llama2 13B Chat (GGUF)",
            "routing_key": "llama_13b_chat_gguf",            
            "use": ["language_model"],        
            "available_precision": { "cpu": ["4-bit", "6-bit"], "cuda": ["4-bit", "6-bit"] },
            "memory_usage": { "4-bit": 11000, "6-bit": 13800 },            
            "model": [{
                "name": "TheBloke/Llama-2-13B-chat-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "llama-2-13b-chat.Q4_0.gguf"
                }
            }],
            "configuration": {
                "model_layers": 43,
                "num_threads": -1,
                "model_type": "chat",
                "max_seq_len": 4096,
                "user_role": "[INST]",
                "ai_role": "[/INST]",
                "stop_on": ["<stop>", "[INST]", "</s>"],
                "system_prompt_format": "{user_role} <<SYS>>{system_prompt}<</SYS>>\n{prompt} {ai_role} {response}",
                "system_message": "You are an helpful assistant.",
                "prompt_format": "{user_role} {prompt} {ai_role} {response}"
            }        
        },
        {
            "label":  "CodeLlama 34B Instruct (GGUF)",
            "routing_key": "llama_34b_instruct_gguf",            
            "use": ["language_model"],        
            "available_precision": { "cpu": ["4-bit", "6-bit"], "cuda": ["4-bit", "6-bit"] },
            "memory_usage": { "4-bit": 22000, "6-bit": 31500 },            
            "model": [{
                "name": "TheBloke/CodeLlama-34B-Instruct-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "codellama-34b-instruct.Q4_K_M.gguf",
                    "6-bit": "codellama-34b-instruct.Q6_K.gguf"
                }
            }],
            "configuration": {
                "model_layers": 51,
                "num_threads": -1,
                "model_type": "instruct",
                "max_seq_len": 16384,
                "user_role": "[INST]",
                "ai_role": "[/INST]",
                "stop_on": ["<stop>", "[INST]", "</s>"],
                "system_message": "",
                "prompt_format": "{user_role} {prompt} {ai_role} {response}\n\n"
            }        
        },
        {
            "label":  "Mistral 7B Instruct (GGUF)",
            "routing_key": "mistral_7b_instruct_gguf",            
            "use": ["language_model"],                    
            "available_precision": { "cuda": ["4-bit", "6-bit"] },
            "memory_usage": { "4-bit": 6900, "6-bit": 11000 },
            "model": [{
                "name": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "mistral-7b-instruct-v0.2.Q4_K_M.gguf",
                    "6-bit": "mistral-7b-instruct-v0.2.Q6_K.gguf"
                }
            }],
            "configuration": {           
                "model_layers": 35,     
                "max_seq_len": 8192,
                "user_role": "[INST]",
                "ai_role": "[/INST]",
                "stop_on": ["<stop>", "### USER:"],
                "system_prompt_format": "[INST] {system_prompt} [/INST] ",
                "system_message": "",
                "prompt_format": "{user_role} {prompt} {ai_role} {response} "
            }
        },                       
        {
            "label":  "Llama2 70B Chat (GGUF)",
            "routing_key": "llama_70b_chat_gguf",            
            "use": ["language_model"],        
            "available_precision": { "cpu": ["4-bit", "5-bit", "6-bit"], "cuda": ["4-bit", "5-bit", "6-bit"] },
            "memory_usage": { "4-bit": 41500, "5-bit": 48000, "6-bit": 60000 },            
            "model": [{
                "name": "TheBloke/Llama-2-70B-chat-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "llama-2-70b-chat.Q4_K_M.gguf",
                    "5-bit": "llama-2-70b-chat.Q5_K_M.gguf",
                    "6-bit": "llama-2-70b-chat.Q6_K.gguf"
                },
                "split": {
                    "6-bit": ["-split-a", "-split-b"]
                }
            }],
            "configuration": {
                "model_layers": 83,
                "num_threads": -1,
                "model_type": "chat",
                "max_seq_len": 4096,
                "user_role": "[INST]",
                "ai_role": "[/INST]",
                "stop_on": ["<stop>", "[INST]", "</s>"],
                "system_prompt_format": "{user_role} <<SYS>>{system_prompt}<</SYS>>\n{prompt} {ai_role} {response}",
                "prompt_format": "{user_role} {prompt} {ai_role} {response}",
                "system_message": "You are an helpful assistant."
            }        
        },
        {
            "label":  "CodeLlama 70B Instruct (GGUF)",
            "routing_key": "codellama_70b_instruct_gguf",            
            "use": ["language_model"],        
            "available_precision": { "cpu": ["4-bit", "5-bit"], "cuda": ["4-bit", "5-bit"] },
            "memory_usage": { "4-bit": 41500, "5-bit": 48000 },            
            "model": [{
                "name": "LoneStriker/CodeLlama-70b-Instruct-hf-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "CodeLlama-70b-Instruct-hf-Q4_K_M.gguf"
                }
            }],
            "configuration": {
                "model_layers": 83,
                "num_threads": -1,
                "model_type": "chat",
                "max_seq_len": 4096,
                "user_role": "Source: user",
                "ai_role": "Source: assistant",
                "stop_on": ["<stop>", "Source:", "Source:"],
                "system_prompt_format": "Source: system\n\n{system_prompt}<step> ",
                "prompt_format": "{user_role}\n\n{prompt}<step> {ai_role} Destination: user\n{response}",
                "system_message": "You are an expert in coding Magento 2"
            }        
        },
        {
            "label":  "Mixtral 8x7B Instruct (GGUF)",
            "routing_key": "mixtral_8x7b_instruct",
            "use": ["language_model", "reasoning_agent"],        
            "available_precision": { "cpu": ["4-bit", "5-bit", "6-bit"], "cuda": ["4-bit", "5-bit", "6-bit"] },
            "memory_usage": { "4-bit": 28000, "5-bit": 33500, "6-bit": 39000 },            
            "model": [{
                "name": "TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF",
                "provider": "huggingface",
                "files": {
                    "4-bit": "mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
                    "5-bit": "mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf",
                    "6-bit": "mixtral-8x7b-instruct-v0.1.Q6_K.gguf"
                }
            }],
            "configuration": {
                "model_layers": 63,
                "num_threads": -1,
                "max_seq_len": 16384,                                
                "user_role": "[INST]",
                "ai_role": "[/INST]",
                "stop_on": ["<stop>", "[/INST]"],
                "system_prompt_format": "[INST] {system_prompt}\nRespond with OK if you understand. [/INST] OK ",
                "system_message": "",
                "prompt_format": "{user_role} {prompt} {ai_role} {response} "
            }        
        }
    ] 
}